===========
ramayanaocr
===========
The culture and heritage of India depicted in the ancient manuscripts are unique in its linguistic and traditional diversity. But, the paucity of skill and expertise in present-day research has posed a threat to the exploration of this textual legacy. In this paper, we aim to create a visual narrative of one of the major epics of Indian Literature-'Ramayana', by summarizing the major topics and linking them with the characters and locations using Artificial Intelligence (AI). Using this research, any person interested in studying these manuscripts can visualize the tenor of the entire script, without an intensive study. 'Ramayana' was originally written in Sanskrit, but modern versions have Sanskrit text with the explanation in Hindi (as most people in India are well versed with Hindi). In this paper, we have divided the Hindi and Sanskrit text and considered only Hindi text for our further research. We have used existing scientific models (that are trained on the Hindi Language) to find events/topics, summaries, characters and locations that are later used to produce a visual narrative of the data. For the evaluation of our results, we have tried to review the understanding of our summaries and topics. We achieved this by providing a part of our input text and its summary as well as topics/events created by our data-pipeline, to 30 people (who are well versed with the Hindi Language). From the survey, it was found that 70$\%$ of the respondents understood the summarised text, while 56$\%$ of the respondents understood the topics clearly,that is generated from our model.

Description
===========

At present, there is a growing enthusiasm among historians and humanitarians for exploring the quintessence of ancient Indian manuscripts. As there exists physical evidence of the events occurring in most of these scriptures, we can gain information about the demography and cultural aspects of ancient India. Also, other important aspects like architecture, medicine, engineering, beliefs, etc. can be studied through these manuscripts. Hence, the underlying notion behind this research is to create a pipeline, where-in any ancient manuscript can be provided as an input, resulting in the formulation of events/topics and summaries from the text, that can provide an overview of that scripture even without any domain knowledge.

For our research we have used one of the two major epics in the ancient Indian history - 'Ramayana'. There exists around 300 versions of 'Ramayana' throughout the world We have used a subset of the 'Valmiki Ramayana' for our research, that is downloaded from the website https://archive.org/details/in.ernet.dli.2015.345471/page/n1/mode/2up in pdf format. This version is an epic tale narrated by Rishi Valmiki (written in Sanskrit) describing the journey of Lord Rama, his wife Sita and brother Lakshmana and how He, i.e., Lord Rama, triumphed over the evil forces of Ravana, the Demon King of Lanka. Not only the characters in this scripture are considered as gods in India, also there exists physical evidence of the events stated in theses scriptures, that can be traced back to present-day locations in India and Sri-Lanka. 
Our downloaded data consists of the original Sanskrit Shlokas followed by its interpretation in Hindi.

From this data we want to produce a visual narrative so that any individual can understand the gist of the entire text without having to read the entire text. Our pseudo-pipeline can be used to reproduce similar results for any Indian manuscript. Our designed pipeline can be divided into five major components. These components are: 
   
1.	Input data (Digitize the data for further processing) - The main aim of this process is to digitize the given input data that is in pdf format.  Firstly, we need to fetch each page and convert it into PNG format. Then these images are fed into an OCR engine that will convert the images into machine-readable text. The OCR engine used for our study is the Py-Tesseract OCR https://github.com/madmaze/pytesseract for Devanagiri Scripts.

2.	Basic Pre-Processing (Preparing the data) - After getting the data in machine-readable text, we need to clean the data, as the OCR'd text might be wrongly read, and if this wrong text is further processed in our pipeline, then the results can be hugely impacted. The first step in this process is to remove the headers from the text document, as they don't provide any value to the data. Now, we have the data with both Sanskrit and Hindi text. We decided to use only Hindi text for further processing, as the Hindi text contains the description of the Sanskrit Sargsthat should provide better results for our Summarization and Topic Modelling models (due to considerable amount of data as compared to Sanskrit text). Hence, the Sanskrit and Hindi texts were separated (using some keyword identifiers) and stored in 2 different files. The Hindi text was then divided into 26 parts based on a word-limit, that would help in creating small summaries and also would make it easier to find events from the divided sub-text. All these summaries and topics/events can then be used to find the quintessence of the entire data. From this step, the data was simultaneously sent into 2 different processes, i.e., one for Text-Summarization and NER Tagging, and other for Topic-Modelling.

3.	Summarization and NER Tagging (creating a concise summary and then finding locations/persons involved in those summaries using NER Tagging) - This step is divided into 2 processes as described below:
•	Summarization(summing up the most important or relevant information from the entire text)- Text Summarization is the process of producing a concise and fluent summary while preserving key information content and overall meaning. There are two types of Summarization techniques i.e., Extractive Summarization and Abstractive Summarization. Extractive Summarization works by identifying important sections of the text and combining them to make a summary. While Abstractive Summarization entails paraphrasing and shortening parts of the source document, thus producing important material in a new way. For our research, we are performing Extractive Summarization using the text-rank algorithm https://datawarrior.wordpress.com/2015/05/20/birdview-2-ranking-everything-an-overview-of-link-analysis-using-pagerank-algorithm/, that is modeled using a combination of sklearn and netorkx open source libraries in python.
•	NER Tagging (tagging persons/locations/organizations etc from the summarised text)- We have used Named entity recognition (NER) algorithm to find and cluster named entities in text into any desired categories such as person names (PER), organizations (ORG), locations (LOC), time expressions, etc . For, training the model the open-source Python library Flair  has been used. Also, to train the model, the Fire 2013- Hindi NER Corpus from AU-KBC research Centre, India was used.

4.	Topic Modeling (finding topics/events from the divided Hindi input text) - Further pre-processing is required to perform Topic Modeling. These pre-processing steps and the Topic Modeling process are described below:
•	Pre-processing for Topic Modeling (preparing the data for Topic Modeling): To perform Topic Modeling on our data, some pre-processing steps are required (remove irrelevant words that might affect the probabilistic LDA model, working on the principle of bag-of-words analysis). Three procedures are performed in this step i.e., lemmatization, stop-words removal and removal of other irrelevant words. Lemmatization is the process of grouping together synonymous words so that they can be inferred as a single word. We performed lemmatization using open-source python library- Stanford NLP. Then some more unused words like prepositions were removed in the 'stop-word removal' process. A list of stop-words was manually created for this research. In the last step for the data-cleaning process, some garbage data like miss-interpreted English words, punctuation's, numbers, etc. were removed.
•	Topic Modeling (finding topics/events): Topic Modeling is the process of identifying topics/events from a given text-input.The "topics" signify the hidden, to be estimated, variable relations that link words in vocabulary and their occurrence in documents. Topic models discover the hidden themes throughout the collection and annotate the documents according to those themes . To perform this the LDA algorithm is used, that finds the probabilistic word-frequencies from the bag-of-words. For our research, we are using the LDA based topic model for the Hindi text of python open-source library -Gensim 

5.	Visualisation (creating a story-line from the available text-summaries, topics/Events and identified locations and characters) - All the above components of our pipe-line are designed using Python, and the visualization of our results is performed using Tableau. The obtained NER-tags were validated and filtered using some validation data-sets. For creating a visual narrative of the above obtained results, two dashboards were built. In one of the dashboards, the locations (retrieved using the NER Tags) were plotted on the map of Indian-Subcontinent and when hovered over a mapped location, the summary linked to that location is displayed. Using this, anyone can be able to create an mental image of the story that was described in its summary. The other dashboard displays the characters (retrieved using the NER Tags) distributed according to their corresponding summary. And, when hovered each character, the topics/events associated with that character are displayed. Using both these dashboards, anyone can easily find the quintessence of the whole script without doing any intensive study on it.



Note
====

Comparing OCR models: Tesseract and Transkribus for Devanagari script.
Please check-out the comparision of OCR for hindi/sanskrit text. 
https://github.com/ramayanaocr/ocr-comparison
